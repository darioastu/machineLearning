{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0KqFzJM66Uj4Hg9kVYNda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darioastu/machineLearning/blob/master/Semana%206.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xoNV0EnTeVN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "np.random.seed(2)\n",
        "qLog[]\n",
        "N_STATES=10\n",
        "ACTIONS=['Izquierda','derecha']\n",
        "EPSILON=0.9\n",
        "ALPHA=0.1\n",
        "GAMMA=0.9\n",
        "MAX_EPISODES =6\n",
        "FRESCH_TIME=0.3\n",
        "\n",
        "\n",
        "def construir_tabla_q(n_states,actions):\n",
        "  table =pd.DataFrame(\n",
        "      np.zeros(n_states,len(actions))),\n",
        "      columns=actions,\n",
        "  )\n",
        "  print('\\r\\ntabla   original:\\n')\n",
        "  print(table)\n",
        "  print('\\n')\n",
        "  return table \n",
        "\n",
        "  def escoger_accion(state,tablaQ):\n",
        "    state_actions =tablaQ.iloc[state,:]\n",
        "\n",
        "    if (np.random.uniform()>EPSILON) or ((state_actions==0).all()):\n",
        "      action_name=np.random.choice(ACTIONS)\n",
        "    else:\n",
        "      action_name = state\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T41nOIjbiDn",
        "outputId": "af878adb-34bc-4e1a-c3d9-054927d6902a"
      },
      "source": [
        "\"\"\"\n",
        "Un ejemplo sencillo de aprendizaje por refuerzo utilizando el método de \n",
        "Q-learning de búsqueda de tablas.\n",
        "Un agente \"o\" está a la izquierda de un mundo unidimensional, \n",
        "el tesoro está en el ubicación más a la derecha.\n",
        "Ejecute este programa y vea cómo el agente mejorará su estrategia de búsqueda.\n",
        "del tesoro.\n",
        "\n",
        "Código original: morvanzhou\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "np.random.seed(2)  # para reproducir de forma consistente en el tiempo\n",
        "qLog =[]\n",
        "\n",
        "N_STATES = 15      # longitud del vector de dimensión-1: procesoQLearning\n",
        "ACTIONS = ['izquierda', 'derecha']     # acciones posibles\n",
        "EPSILON = 1      # politica codiciosa\n",
        "ALPHA = 0.1        # tasa de aprendizaje\n",
        "GAMMA = 0.9        # factor de castigo\n",
        "MAX_EPISODES = 9  # episodios posibles\n",
        "FRESH_TIME = 0.5   # primera vez de un movimiento\n",
        "\n",
        "\n",
        "def construir_tabla_q(n_states, actions):\n",
        "    table = pd.DataFrame(\n",
        "        np.zeros((n_states, len(actions))),     \n",
        "        # valores iniciales de la tablaQ \n",
        "        columns=actions,    # nombre de la acción\n",
        "    )\n",
        "    print('\\r\\ntablaQ original:\\n')\n",
        "    print(table)          # mostrar tabla\n",
        "    print('\\n')\n",
        "    return table\n",
        "\n",
        "\n",
        "def escoger_accion(state, tablaQ):\n",
        "    # Esto es como se escoge la acción\n",
        "    state_actions = tablaQ.iloc[state, :]\n",
        "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  \n",
        "      # accion sin codicia o la acción no tiene valoración\n",
        "        action_name = np.random.choice(ACTIONS)\n",
        "    else:   # actuar codiciosamente \n",
        "        action_name = state_actions.idxmax()    \n",
        "        # remplazar argmax por idxmax. \n",
        "        # El argmax es diferente función en pandas actuales pep 3.7\n",
        "    return action_name\n",
        "\n",
        "\n",
        "def analizar_ambiente_elegido(S, A):\n",
        "    # Esto es como el agente intercatua con el ambiente\n",
        "    if A == 'derecha':    # mover a la  derecha\n",
        "        if S == N_STATES - 2:   # terminar\n",
        "            S_ = 'terminal'\n",
        "            R = 1              # R reward / ganancia\n",
        "        else:\n",
        "            S_ = S + 1\n",
        "            R = 0\n",
        "    else:   # mover a la izquierda\n",
        "        R = 0\n",
        "        if S == 0:\n",
        "            S_ = S  # alcanzar el muro\n",
        "        else:\n",
        "            S_ = S - 1\n",
        "    return S_, R\n",
        "\n",
        "\n",
        "def actualizar_ambiente(S, episode, step_counter):\n",
        "    # Esto es como el ambiente es actualizado\n",
        "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' es el ambiente\n",
        "    if S == 'terminal':\n",
        "        interaction = 'Episodio %s: pasos totales = %s' % (episode + 1, step_counter)\n",
        "        qLog.append(interaction)\n",
        "        print('\\r{}'.format(interaction), end='')\n",
        "        time.sleep(1)\n",
        "        print('\\r                                ', end='')\n",
        "    else:\n",
        "        env_list[S] = 'o'\n",
        "        interaction = ''.join(env_list)\n",
        "        print('\\r{}'.format(interaction), end='')\n",
        "        time.sleep(FRESH_TIME)\n",
        "\n",
        "\n",
        "def procesoQLearning():\n",
        "    # El proceso que forma el aprendizaje por procesoQLearning\n",
        "    tablaQ = construir_tabla_q(N_STATES, ACTIONS)\n",
        "    for episode in range(MAX_EPISODES):\n",
        "        step_counter = 0\n",
        "        S = 0\n",
        "        is_terminated = False\n",
        "        actualizar_ambiente(S, episode, step_counter)\n",
        "        while not is_terminated:\n",
        "\n",
        "            A = escoger_accion(S, tablaQ)\n",
        "            S_, R = analizar_ambiente_elegido(S, A)  # tomar acción y obtener el siguiente estado y premio (si lo hay)\n",
        "            q_predict = tablaQ.loc[S, A]\n",
        "            if S_ != 'terminal':\n",
        "                q_target = R + GAMMA * tablaQ.iloc[S_, :].max()   # el siguiente estado no es el final\n",
        "            else:\n",
        "                q_target = R     # el siguiente estado es el final\n",
        "                is_terminated = True    # terminar este episodio\n",
        "\n",
        "            tablaQ.loc[S, A] += ALPHA * (q_target - q_predict)  # actualizar\n",
        "            S = S_  # moverse al siguiente estado\n",
        "\n",
        "            actualizar_ambiente(S, episode, step_counter+1)\n",
        "            step_counter += 1\n",
        "    return tablaQ\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tablaQ = procesoQLearning()\n",
        "    print('\\nLogs de proceso')\n",
        "    for m in range(len(qLog)):\n",
        "      print(qLog[m]),\n",
        "    \n",
        "    print('\\n\\r\\ntablaQ final:\\n')\n",
        "    print(tablaQ)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "tablaQ original:\n",
            "\n",
            "    izquierda  derecha\n",
            "0         0.0      0.0\n",
            "1         0.0      0.0\n",
            "2         0.0      0.0\n",
            "3         0.0      0.0\n",
            "4         0.0      0.0\n",
            "5         0.0      0.0\n",
            "6         0.0      0.0\n",
            "7         0.0      0.0\n",
            "8         0.0      0.0\n",
            "9         0.0      0.0\n",
            "10        0.0      0.0\n",
            "11        0.0      0.0\n",
            "12        0.0      0.0\n",
            "13        0.0      0.0\n",
            "14        0.0      0.0\n",
            "\n",
            "\n",
            "                                \n",
            "Logs de proceso\n",
            "Episodio 1: pasos totales = 91\n",
            "Episodio 2: pasos totales = 147\n",
            "Episodio 3: pasos totales = 217\n",
            "Episodio 4: pasos totales = 48\n",
            "Episodio 5: pasos totales = 28\n",
            "Episodio 6: pasos totales = 29\n",
            "Episodio 7: pasos totales = 24\n",
            "Episodio 8: pasos totales = 21\n",
            "Episodio 9: pasos totales = 46\n",
            "\n",
            "\n",
            "tablaQ final:\n",
            "\n",
            "    izquierda       derecha\n",
            "0         0.0  0.000000e+00\n",
            "1         0.0  0.000000e+00\n",
            "2         0.0  0.000000e+00\n",
            "3         0.0  0.000000e+00\n",
            "4         0.0  0.000000e+00\n",
            "5         0.0  4.304672e-10\n",
            "6         0.0  3.922035e-08\n",
            "7         0.0  1.593260e-06\n",
            "8         0.0  3.792953e-05\n",
            "9         0.0  5.845326e-04\n",
            "10        0.0  6.073368e-03\n",
            "11        0.0  4.290743e-02\n",
            "12        0.0  2.026431e-01\n",
            "13        0.0  6.125795e-01\n",
            "14        0.0  0.000000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}